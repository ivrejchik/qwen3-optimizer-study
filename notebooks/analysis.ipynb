{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3 Optimizer Comparison Analysis\n",
    "\n",
    "This notebook provides an interactive analysis of the optimizer comparison results.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We compare three optimizers:\n",
    "- **AdamW**: Adaptive moment estimation with weight decay\n",
    "- **SGD + Momentum**: Stochastic gradient descent with momentum\n",
    "- **AdaBound**: Smooth transition from Adam to SGD\n",
    "\n",
    "All models are fine-tuned on CommonsenseQA using LoRA (Low-Rank Adaptation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"üìä Analysis environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_path = \"../results/results.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(results_path)\n",
    "    print(f\"‚úÖ Loaded {len(df)} results\")\n",
    "    print(f\"Models: {', '.join(df['model'].tolist())}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Results file not found. Run the evaluation first.\")\n",
    "    print(\"Example data for demonstration:\")\n",
    "    \n",
    "    # Create example data\n",
    "    df = pd.DataFrame({\n",
    "        'model': ['baseline', 'adamw', 'sgd', 'adabound'],\n",
    "        'accuracy': [0.6245, 0.6892, 0.6634, 0.6978],\n",
    "        'items_per_second': [2.34, 2.12, 2.28, 2.05],\n",
    "        'evaluation_time_seconds': [527.3, 581.2, 540.8, 601.4],\n",
    "        'gpu_memory_used_mb': [8234, 8456, 8321, 8492]\n",
    "    })\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nüìã Dataset Info:\")\n",
    "display(df.info())\n",
    "\n",
    "print(\"\\nüìä Summary Statistics:\")\n",
    "display(df.describe().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results table\n",
    "print(\"üéØ Complete Results:\")\n",
    "display(df.style.format({\n",
    "    'accuracy': '{:.4f}',\n",
    "    'items_per_second': '{:.2f}',\n",
    "    'evaluation_time_seconds': '{:.1f}',\n",
    "    'gpu_memory_used_mb': '{:.0f}'\n",
    "}).background_gradient(subset=['accuracy'], cmap='RdYlGn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot\n",
    "colors = ['#ff7f0e' if model == 'baseline' else '#1f77b4' for model in df['model']]\n",
    "bars = ax1.bar(df['model'], df['accuracy'], color=colors, alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, df['accuracy']):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Model Accuracy Comparison')\n",
    "ax1.set_ylim(0, max(df['accuracy']) * 1.1)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Horizontal bar plot (sorted)\n",
    "df_sorted = df.sort_values('accuracy')\n",
    "bars2 = ax2.barh(df_sorted['model'], df_sorted['accuracy'], \n",
    "                 color=['#ff7f0e' if model == 'baseline' else '#1f77b4' for model in df_sorted['model']],\n",
    "                 alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add value labels\n",
    "for i, (model, acc) in enumerate(zip(df_sorted['model'], df_sorted['accuracy'])):\n",
    "    ax2.text(acc + 0.002, i, f'{acc:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Accuracy')\n",
    "ax2.set_title('Model Accuracy (Ranked)')\n",
    "ax2.set_xlim(0, max(df['accuracy']) * 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best model\n",
    "best_model = df.loc[df['accuracy'].idxmax()]\n",
    "print(f\"üèÜ Best Model: {best_model['model']} with {best_model['accuracy']:.4f} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Metrics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].bar(df['model'], df['accuracy'], color='skyblue', alpha=0.8, edgecolor='navy')\n",
    "axes[0, 0].set_title('Accuracy by Optimizer', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Speed\n",
    "axes[0, 1].bar(df['model'], df['items_per_second'], color='lightgreen', alpha=0.8, edgecolor='darkgreen')\n",
    "axes[0, 1].set_title('Inference Speed by Optimizer', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Items per Second')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Evaluation time\n",
    "axes[1, 0].bar(df['model'], df['evaluation_time_seconds'], color='salmon', alpha=0.8, edgecolor='darkred')\n",
    "axes[1, 0].set_title('Evaluation Time by Optimizer', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Time (seconds)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# GPU Memory (if available)\n",
    "if 'gpu_memory_used_mb' in df.columns:\n",
    "    axes[1, 1].bar(df['model'], df['gpu_memory_used_mb'], color='gold', alpha=0.8, edgecolor='orange')\n",
    "    axes[1, 1].set_title('GPU Memory Usage by Optimizer', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Memory (MB)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'GPU Memory\\nData Not Available', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=14)\n",
    "    axes[1, 1].set_title('GPU Memory Usage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improvement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement over baseline\n",
    "baseline_row = df[df['model'] == 'baseline']\n",
    "\n",
    "if not baseline_row.empty:\n",
    "    baseline_acc = baseline_row['accuracy'].iloc[0]\n",
    "    \n",
    "    # Calculate improvements\n",
    "    improvements = []\n",
    "    for _, row in df.iterrows():\n",
    "        if row['model'] != 'baseline':\n",
    "            improvement = (row['accuracy'] - baseline_acc) * 100\n",
    "            improvements.append({\n",
    "                'model': row['model'],\n",
    "                'improvement_pct': improvement,\n",
    "                'accuracy': row['accuracy']\n",
    "            })\n",
    "    \n",
    "    if improvements:\n",
    "        imp_df = pd.DataFrame(improvements)\n",
    "        \n",
    "        # Plot improvements\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        colors = ['#2ca02c' if imp > 0 else '#d62728' for imp in imp_df['improvement_pct']]\n",
    "        bars = plt.bar(imp_df['model'], imp_df['improvement_pct'], color=colors, alpha=0.8, edgecolor='black')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (model, imp) in enumerate(zip(imp_df['model'], imp_df['improvement_pct'])):\n",
    "            plt.text(i, imp + 0.1 if imp > 0 else imp - 0.1, f'{imp:+.2f}%', \n",
    "                    ha='center', va='bottom' if imp > 0 else 'top', fontweight='bold')\n",
    "        \n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.xlabel('Optimizer')\n",
    "        plt.ylabel('Accuracy Improvement (%)')\n",
    "        plt.title(f'Accuracy Improvement over Baseline\\n(Baseline: {baseline_acc:.4f})', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary table\n",
    "        print(\"üìà Improvement Summary:\")\n",
    "        display(imp_df.style.format({\n",
    "            'improvement_pct': '{:+.2f}%',\n",
    "            'accuracy': '{:.4f}'\n",
    "        }).background_gradient(subset=['improvement_pct'], cmap='RdYlGn'))\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No baseline model found for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Efficiency Analysis: Accuracy vs Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Accuracy vs Speed\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create scatter plot\n",
    "colors = ['#ff7f0e' if model == 'baseline' else '#1f77b4' for model in df['model']]\n",
    "sizes = [150 if model == 'baseline' else 100 for model in df['model']]\n",
    "\n",
    "scatter = plt.scatter(df['items_per_second'], df['accuracy'], \n",
    "                     s=sizes, c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "\n",
    "# Add labels for each point\n",
    "for _, row in df.iterrows():\n",
    "    plt.annotate(row['model'], \n",
    "                (row['items_per_second'], row['accuracy']),\n",
    "                xytext=(10, 10), textcoords='offset points', \n",
    "                fontsize=11, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Add quadrant lines\n",
    "plt.axhline(y=df['accuracy'].mean(), color='gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "plt.axvline(x=df['items_per_second'].mean(), color='gray', linestyle='--', alpha=0.5, linewidth=2)\n",
    "\n",
    "# Add quadrant labels\n",
    "plt.text(0.02, 0.98, 'High Accuracy\\nLow Speed', transform=plt.gca().transAxes, \n",
    "         ha='left', va='top', fontsize=10, \n",
    "         bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "plt.text(0.98, 0.98, 'High Accuracy\\nHigh Speed', transform=plt.gca().transAxes, \n",
    "         ha='right', va='top', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "plt.text(0.02, 0.02, 'Low Accuracy\\nLow Speed', transform=plt.gca().transAxes, \n",
    "         ha='left', va='bottom', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.7))\n",
    "\n",
    "plt.text(0.98, 0.02, 'Low Accuracy\\nHigh Speed', transform=plt.gca().transAxes, \n",
    "         ha='right', va='bottom', fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "\n",
    "plt.xlabel('Inference Speed (items/second)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Efficiency Analysis: Accuracy vs Speed Trade-off', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = df['items_per_second'].corr(df['accuracy'])\n",
    "print(f\"üîÑ Speed-Accuracy Correlation: {correlation:.3f}\")\n",
    "if abs(correlation) > 0.5:\n",
    "    trend = \"positive\" if correlation > 0 else \"negative\"\n",
    "    print(f\"üìä Strong {trend} correlation detected!\")\n",
    "else:\n",
    "    print(\"üìä Weak correlation - independent performance factors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"üìä Statistical Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Accuracy statistics\n",
    "acc_stats = df['accuracy'].describe()\n",
    "print(f\"\\nüéØ Accuracy Statistics:\")\n",
    "print(f\"   Mean: {acc_stats['mean']:.4f}\")\n",
    "print(f\"   Std:  {acc_stats['std']:.4f}\")\n",
    "print(f\"   Min:  {acc_stats['min']:.4f} ({df.loc[df['accuracy'].idxmin(), 'model']})\")\n",
    "print(f\"   Max:  {acc_stats['max']:.4f} ({df.loc[df['accuracy'].idxmax(), 'model']})\")\n",
    "print(f\"   Range: {acc_stats['max'] - acc_stats['min']:.4f}\")\n",
    "\n",
    "# Speed statistics\n",
    "speed_stats = df['items_per_second'].describe()\n",
    "print(f\"\\n‚ö° Speed Statistics:\")\n",
    "print(f\"   Mean: {speed_stats['mean']:.2f} items/sec\")\n",
    "print(f\"   Std:  {speed_stats['std']:.2f}\")\n",
    "print(f\"   Min:  {speed_stats['min']:.2f} ({df.loc[df['items_per_second'].idxmin(), 'model']})\")\n",
    "print(f\"   Max:  {speed_stats['max']:.2f} ({df.loc[df['items_per_second'].idxmax(), 'model']})\")\n",
    "\n",
    "# Variability analysis\n",
    "acc_cv = (acc_stats['std'] / acc_stats['mean']) * 100\n",
    "speed_cv = (speed_stats['std'] / speed_stats['mean']) * 100\n",
    "\n",
    "print(f\"\\nüìà Coefficient of Variation:\")\n",
    "print(f\"   Accuracy: {acc_cv:.2f}%\")\n",
    "print(f\"   Speed: {speed_cv:.2f}%\")\n",
    "\n",
    "if acc_cv < 5:\n",
    "    print(\"   ‚Üí Low accuracy variability (models perform similarly)\")\n",
    "elif acc_cv > 15:\n",
    "    print(\"   ‚Üí High accuracy variability (significant differences)\")\n",
    "else:\n",
    "    print(\"   ‚Üí Moderate accuracy variability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights\n",
    "print(\"üîç KEY INSIGHTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Best performing model\n",
    "best_model = df.loc[df['accuracy'].idxmax()]\n",
    "print(f\"\\nüèÜ BEST ACCURACY: {best_model['model'].upper()}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {best_model['accuracy']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Speed: {best_model['items_per_second']:.2f} items/sec\")\n",
    "\n",
    "# Fastest model\n",
    "fastest_model = df.loc[df['items_per_second'].idxmax()]\n",
    "print(f\"\\n‚ö° FASTEST MODEL: {fastest_model['model'].upper()}\")\n",
    "print(f\"   ‚Ä¢ Speed: {fastest_model['items_per_second']:.2f} items/sec\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {fastest_model['accuracy']:.4f}\")\n",
    "\n",
    "# Most balanced model\n",
    "df['normalized_acc'] = (df['accuracy'] - df['accuracy'].min()) / (df['accuracy'].max() - df['accuracy'].min())\n",
    "df['normalized_speed'] = (df['items_per_second'] - df['items_per_second'].min()) / (df['items_per_second'].max() - df['items_per_second'].min())\n",
    "df['balance_score'] = (df['normalized_acc'] + df['normalized_speed']) / 2\n",
    "\n",
    "balanced_model = df.loc[df['balance_score'].idxmax()]\n",
    "print(f\"\\n‚öñÔ∏è MOST BALANCED: {balanced_model['model'].upper()}\")\n",
    "print(f\"   ‚Ä¢ Balance Score: {balanced_model['balance_score']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {balanced_model['accuracy']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Speed: {balanced_model['items_per_second']:.2f} items/sec\")\n",
    "\n",
    "# Baseline comparison\n",
    "baseline = df[df['model'] == 'baseline']\n",
    "if not baseline.empty:\n",
    "    baseline_acc = baseline['accuracy'].iloc[0]\n",
    "    improved_models = df[df['accuracy'] > baseline_acc]\n",
    "    \n",
    "    print(f\"\\nüìà BASELINE COMPARISON:\")\n",
    "    print(f\"   ‚Ä¢ Baseline accuracy: {baseline_acc:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Models beating baseline: {len(improved_models) - 1}\")  # -1 to exclude baseline itself\n",
    "    \n",
    "    if len(improved_models) > 1:\n",
    "        avg_improvement = ((improved_models['accuracy'].mean() - baseline_acc) * 100)\n",
    "        print(f\"   ‚Ä¢ Average improvement: {avg_improvement:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° RECOMMENDATIONS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if best_model['model'] != 'baseline':\n",
    "    print(f\"\\n1. üéØ FOR HIGHEST ACCURACY:\")\n",
    "    print(f\"   ‚Üí Use {best_model['model'].upper()} optimizer\")\n",
    "    print(f\"   ‚Üí Expected accuracy: {best_model['accuracy']:.4f}\")\n",
    "\n",
    "if fastest_model['model'] != best_model['model']:\n",
    "    print(f\"\\n2. ‚ö° FOR FASTEST INFERENCE:\")\n",
    "    print(f\"   ‚Üí Use {fastest_model['model'].upper()} optimizer\")\n",
    "    print(f\"   ‚Üí Expected speed: {fastest_model['items_per_second']:.2f} items/sec\")\n",
    "\n",
    "print(f\"\\n3. ‚öñÔ∏è FOR BALANCED PERFORMANCE:\")\n",
    "print(f\"   ‚Üí Use {balanced_model['model'].upper()} optimizer\")\n",
    "print(f\"   ‚Üí Good trade-off between accuracy and speed\")\n",
    "\n",
    "print(f\"\\n4. üî¨ FOR FURTHER RESEARCH:\")\n",
    "print(f\"   ‚Üí Try ensemble methods combining top performers\")\n",
    "print(f\"   ‚Üí Experiment with learning rate schedules\")\n",
    "print(f\"   ‚Üí Consider longer training with best optimizer\")\n",
    "\n",
    "print(f\"\\n5. üè≠ FOR PRODUCTION:\")\n",
    "print(f\"   ‚Üí Consider your accuracy vs speed requirements\")\n",
    "print(f\"   ‚Üí Monitor resource usage in your specific environment\")\n",
    "print(f\"   ‚Üí Test with your actual data distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"../results/notebook_analysis\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save processed dataframe\n",
    "df_export = df[['model', 'accuracy', 'items_per_second', 'evaluation_time_seconds', 'balance_score']].copy()\n",
    "df_export = df_export.round(4)\n",
    "df_export.to_csv(output_dir / \"processed_results.csv\", index=False)\n",
    "\n",
    "# Save insights as markdown\n",
    "insights_md = f\"\"\"# Analysis Insights\n",
    "\n",
    "## Best Performing Models\n",
    "\n",
    "- **Highest Accuracy**: {best_model['model']} ({best_model['accuracy']:.4f})\n",
    "- **Fastest Speed**: {fastest_model['model']} ({fastest_model['items_per_second']:.2f} items/sec)\n",
    "- **Most Balanced**: {balanced_model['model']} (score: {balanced_model['balance_score']:.3f})\n",
    "\n",
    "## Statistical Summary\n",
    "\n",
    "- **Mean Accuracy**: {df['accuracy'].mean():.4f} ¬± {df['accuracy'].std():.4f}\n",
    "- **Mean Speed**: {df['items_per_second'].mean():.2f} ¬± {df['items_per_second'].std():.2f} items/sec\n",
    "- **Speed-Accuracy Correlation**: {df['items_per_second'].corr(df['accuracy']):.3f}\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **For production use**: {balanced_model['model']} provides the best overall balance\n",
    "2. **For research**: {best_model['model']} achieves highest accuracy\n",
    "3. **For real-time applications**: {fastest_model['model']} offers fastest inference\n",
    "\"\"\"\n",
    "\n",
    "with open(output_dir / \"insights.md\", \"w\") as f:\n",
    "    f.write(insights_md)\n",
    "\n",
    "print(f\"üìÅ Results exported to: {output_dir}\")\n",
    "print(\"   ‚Ä¢ processed_results.csv\")\n",
    "print(\"   ‚Ä¢ insights.md\")\n",
    "print(\"\\n‚úÖ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 